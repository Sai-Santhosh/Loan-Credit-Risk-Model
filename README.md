# ğŸ¦ Credit Risk Prediction Pipeline

<div align="center">

![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)
![AWS](https://img.shields.io/badge/AWS-Cloud%20Native-orange.svg)
![MLflow](https://img.shields.io/badge/MLflow-Experiment%20Tracking-blue.svg)
![License](https://img.shields.io/badge/License-MIT-green.svg)
![CI/CD](https://img.shields.io/badge/CI%2FCD-GitHub%20Actions-black.svg)

**Production-grade machine learning pipeline for credit risk assessment**

[Features](#-features) â€¢ [Architecture](#-architecture) â€¢ [Quick Start](#-quick-start) â€¢ [AWS Deployment](#-aws-deployment) â€¢ [Documentation](#-documentation)

</div>

---

## ğŸ“‹ Overview

This repository contains a comprehensive, production-ready ML pipeline for predicting credit risk in loan applications. Built with enterprise-grade standards, it features end-to-end automation from data ingestion to model deployment, leveraging AWS services and MLOps best practices.

### ğŸ¯ Business Problem

Predict whether a loan applicant is likely to **default** (Charged Off) or **fully repay** (Fully Paid) their loan, enabling:
- Automated credit decisioning
- Risk-based pricing
- Portfolio risk monitoring
- Regulatory compliance (Fair Lending)

### ğŸ“Š Model Performance

| Metric | LightGBM | XGBoost |
|--------|----------|---------|
| AUC-ROC | **0.718** | 0.724 |
| Accuracy | 69.4% | 66.4% |
| Precision | 88% | 89% |
| Recall | 72% | 67% |
| F1 Score | 0.79 | 0.76 |

---

## âœ¨ Features

### ğŸ”§ Core Capabilities

- **End-to-End ML Pipeline**: Data ingestion â†’ Preprocessing â†’ Training â†’ Evaluation â†’ Deployment
- **Multiple Model Support**: LightGBM and XGBoost with hyperparameter tuning
- **Model Interpretability**: SHAP-based feature importance and prediction explanations
- **Production Code Standards**: OOP design, type hints, comprehensive logging, unit tests

### â˜ï¸ AWS Integration

| Service | Purpose |
|---------|---------|
| **S3** | Data lake for raw/processed data and model artifacts |
| **Lambda** | Real-time inference API and data ingestion triggers |
| **Glue** | ETL jobs for large-scale data processing |
| **Athena** | SQL queries on data lake |
| **SageMaker** | Model training and endpoint deployment |
| **Redshift** | Data warehousing and analytics |
| **CloudFormation** | Infrastructure as Code |
| **Step Functions** | ML workflow orchestration |

### ğŸ“ˆ MLOps Features

- **Experiment Tracking**: MLflow with DagsHub integration
- **Model Registry**: Version control for models with staging/production promotion
- **Feature Store**: Offline/online feature serving
- **Monitoring**: Prometheus metrics and CloudWatch integration

---

## ğŸ— Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        DATA SOURCES                              â”‚
â”‚  (CSV Files, APIs, Databases, Streaming)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AWS S3 (Data Lake)                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚ Raw Data â”‚  â”‚ Processed    â”‚  â”‚ Artifacts  â”‚                â”‚
â”‚  â”‚          â”‚  â”‚ Data         â”‚  â”‚ & Models   â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚             â”‚             â”‚
        â–¼             â–¼             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AWS Glue  â”‚  â”‚ AWS       â”‚  â”‚ AWS       â”‚
â”‚ ETL Jobs  â”‚  â”‚ Athena    â”‚  â”‚ Redshift  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚             â”‚             â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ML TRAINING PIPELINE                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚ Feature    â”‚  â”‚ Model      â”‚  â”‚ HPO with   â”‚                â”‚
â”‚  â”‚ Engineeringâ”‚â”€â–¶â”‚ Training   â”‚â”€â–¶â”‚ Optuna     â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚         â”‚                               â”‚                       â”‚
â”‚         â–¼                               â–¼                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚ MLflow/    â”‚                 â”‚ Model      â”‚                 â”‚
â”‚  â”‚ DagsHub    â”‚                 â”‚ Registry   â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DEPLOYMENT & SERVING                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚ AWS Lambda â”‚  â”‚ SageMaker  â”‚  â”‚ API        â”‚                â”‚
â”‚  â”‚ Functions  â”‚  â”‚ Endpoints  â”‚  â”‚ Gateway    â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Project Structure

```
credit-risk-prediction/
â”‚
â”œâ”€â”€ ğŸ“ config/                    # Configuration files
â”‚   â”œâ”€â”€ config.yaml              # Main configuration
â”‚   â””â”€â”€ logging_config.py        # Logging setup
â”‚
â”œâ”€â”€ ğŸ“ src/                       # Source code
â”‚   â”œâ”€â”€ data/                    # Data loading & transformation
â”‚   â”‚   â”œâ”€â”€ data_loader.py       # S3, Redshift, local loaders
â”‚   â”‚   â”œâ”€â”€ data_validator.py    # Schema validation
â”‚   â”‚   â””â”€â”€ data_transformer.py  # Preprocessing pipeline
â”‚   â”œâ”€â”€ features/                # Feature engineering
â”‚   â”‚   â”œâ”€â”€ feature_engineering.py
â”‚   â”‚   â””â”€â”€ feature_store.py
â”‚   â”œâ”€â”€ models/                  # Model implementations
â”‚   â”‚   â”œâ”€â”€ base_model.py
â”‚   â”‚   â”œâ”€â”€ lgbm_model.py
â”‚   â”‚   â”œâ”€â”€ xgboost_model.py
â”‚   â”‚   â””â”€â”€ model_registry.py
â”‚   â”œâ”€â”€ training/                # Training utilities
â”‚   â”‚   â”œâ”€â”€ trainer.py           # MLflow-integrated trainer
â”‚   â”‚   â””â”€â”€ hyperparameter_tuner.py
â”‚   â”œâ”€â”€ evaluation/              # Model evaluation
â”‚   â”‚   â”œâ”€â”€ evaluator.py
â”‚   â”‚   â””â”€â”€ shap_analyzer.py
â”‚   â””â”€â”€ pipeline/                # End-to-end pipelines
â”‚       â”œâ”€â”€ training_pipeline.py
â”‚       â””â”€â”€ inference_pipeline.py
â”‚
â”œâ”€â”€ ğŸ“ aws/                       # AWS infrastructure
â”‚   â”œâ”€â”€ lambda/                  # Lambda function handlers
â”‚   â”œâ”€â”€ glue/                    # Glue ETL scripts
â”‚   â”œâ”€â”€ sagemaker/               # SageMaker jobs
â”‚   â””â”€â”€ cloudformation/          # IaC templates
â”‚
â”œâ”€â”€ ğŸ“ notebooks/                 # Jupyter notebooks
â”‚   â”œâ”€â”€ 00_unclean_original.ipynb
â”‚   â”œâ”€â”€ 01_data_ingestion.ipynb
â”‚   â”œâ”€â”€ 02_exploratory_data_analysis.ipynb
â”‚   â”œâ”€â”€ 03_data_preprocessing.ipynb
â”‚   â”œâ”€â”€ 04_feature_engineering.ipynb
â”‚   â”œâ”€â”€ 05_model_training.ipynb
â”‚   â”œâ”€â”€ 06_hyperparameter_tuning.ipynb
â”‚   â”œâ”€â”€ 07_model_evaluation.ipynb
â”‚   â””â”€â”€ 08_shap_analysis.ipynb
â”‚
â”œâ”€â”€ ğŸ“ scripts/                   # CLI scripts
â”‚   â”œâ”€â”€ train.py
â”‚   â”œâ”€â”€ evaluate.py
â”‚   â””â”€â”€ run_pipeline.py
â”‚
â”œâ”€â”€ ğŸ“ tests/                     # Test suite
â”‚   â”œâ”€â”€ conftest.py
â”‚   â”œâ”€â”€ test_data_loader.py
â”‚   â”œâ”€â”€ test_models.py
â”‚   â””â”€â”€ test_pipeline.py
â”‚
â”œâ”€â”€ ğŸ“ data/                      # Data directories
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ processed/
â”‚   â””â”€â”€ artifacts/
â”‚
â”œâ”€â”€ Dockerfile                   # Container definition
â”œâ”€â”€ docker-compose.yml           # Multi-container setup
â”œâ”€â”€ Makefile                     # Build automation
â”œâ”€â”€ requirements.txt             # Python dependencies
â””â”€â”€ setup.py                     # Package configuration
```

---

## ğŸš€ Quick Start

### Prerequisites

- Python 3.9+
- AWS Account (for cloud deployment)
- DagsHub Account (for experiment tracking)

### Installation

```bash
# Clone the repository
git clone https://github.com/your-username/credit-risk-prediction.git
cd credit-risk-prediction

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Install package in development mode
pip install -e .
```

### Configuration

```bash
# Copy environment template
cp env.example .env

# Edit configuration
nano .env  # Add your AWS credentials and DagsHub token
```

### Running the Pipeline

```bash
# Using Makefile
make train                    # Train model
make evaluate                 # Evaluate model
make run-pipeline             # Full pipeline

# Using Python scripts
python scripts/train.py --model lightgbm --tune
python scripts/evaluate.py --shap
python scripts/run_pipeline.py --dagshub-repo username/credit-risk
```

### Using Docker

```bash
# Build and run
docker-compose up -d

# Access services
# API: http://localhost:8000
# MLflow: http://localhost:5000
# Jupyter: http://localhost:8888
```

---

## â˜ï¸ AWS Deployment

### Deploy Infrastructure

```bash
# Deploy CloudFormation stack
aws cloudformation deploy \
    --template-file aws/cloudformation/infrastructure.yaml \
    --stack-name credit-risk-ml-pipeline \
    --parameter-overrides Environment=production \
    --capabilities CAPABILITY_NAMED_IAM

# Upload Glue scripts
aws s3 cp aws/glue/etl_job.py s3://credit-risk-ml-pipeline/scripts/glue/

# Deploy Lambda functions
make aws-lambda
```

### Run ETL Pipeline

```bash
# Trigger Glue job
aws glue start-job-run --job-name credit-risk-etl-job

# Check status
aws glue get-job-run --job-name credit-risk-etl-job --run-id <run-id>
```

### Deploy Model to SageMaker

```bash
# Train on SageMaker
python aws/sagemaker/training_job.py --instance-type ml.m5.xlarge

# Deploy endpoint
python aws/sagemaker/deploy_endpoint.py
```

---

## ğŸ“Š MLflow & DagsHub Integration

### Setup DagsHub

```python
import dagshub
import mlflow

# Initialize DagsHub
dagshub.init(repo_owner="your-username", repo_name="credit-risk", mlflow=True)

# MLflow will now track to DagsHub
mlflow.set_experiment("credit-risk-prediction")
```

### Track Experiments

```bash
# Training with MLflow tracking
python scripts/train.py \
    --dagshub-repo your-username/credit-risk \
    --experiment-name credit-risk-v2

# View experiments
mlflow ui
# Or visit: https://dagshub.com/your-username/credit-risk.mlflow
```

---

## ğŸ§ª Testing

```bash
# Run all tests
pytest tests/ -v

# Run with coverage
pytest tests/ -v --cov=src --cov-report=html

# Run specific tests
pytest tests/test_models.py -v
```

---

## ğŸ“ˆ Model Features

### Input Features (19 total)

| Feature | Type | Description |
|---------|------|-------------|
| `term` | Categorical | Loan term (36 or 60 months) |
| `int_rate` | Numerical | Interest rate |
| `grade` | Categorical | Loan grade (A-G) |
| `emp_length` | Categorical | Employment length |
| `home_ownership` | Categorical | Home ownership status |
| `annual_inc` | Numerical | Annual income |
| `verification_status` | Categorical | Income verification status |
| `purpose` | Categorical | Loan purpose |
| `dti` | Numerical | Debt-to-income ratio |
| `revol_util` | Numerical | Revolving utilization |
| `loan_to_income` | Engineered | Loan amount / Annual income |
| `total_interest_owed` | Engineered | Loan Ã— Interest rate |
| `installment_to_income_ratio` | Engineered | Payment / Monthly income |
| `active_credit_pct` | Engineered | Open accounts / Total accounts |
| `credit_age` | Engineered | Years since first credit |

### Top SHAP Feature Importance

1. **DTI** - Debt-to-income ratio
2. **Revolving Utilization** - Credit utilization
3. **Interest Rate** - Loan interest rate
4. **Annual Income** - Borrower income
5. **Active Credit %** - Credit account ratio

---

## ğŸ”’ Security

- All credentials stored in environment variables
- AWS IAM roles with least-privilege access
- S3 bucket encryption enabled
- VPC endpoints for private connectivity
- API authentication via API keys

---

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“§ Contact

**Data Science Team**
- Email: datascience@company.com
- GitHub: [@your-username](https://github.com/your-username)

---

<div align="center">

**Built with â¤ï¸ for the Financial Services Industry**

*Transforming complex banking processes into intelligent software solutions*

</div>
